= {cdc_cass} FAQs

The following are frequently asked questions about {cdc_cass_first} and its features.

== What is {cdc_cass}?

The {cdc_cass} is a an open-source product from {company}.

With {cdc_cass}, updates to data in {cass} or {dse} are put into a Pulsar topic, which in turn can write the data to external targets such as Elasticsearch, Snowflake, and other platforms.
The {csc_pulsar_first} component has a one-to-one correspondence between a {cass-short} table and a single Pulsar topic.

== Is {cdc_cass} an open-source project? Where can I find the repository?

Yes, {cdc_cass} is open source software under the Apache 2.0 license.
You can find the source code on the https://github.com/datastax/cdc-apache-cassandra[{company} {cdc_cass} GitHub repository].

== What does {cdc_cass} provide that I cannot get with open-source Apache Pulsar?

In effect, the {cdc_cass} implements the reverse of Apache Pulsar or {company} {cass-short} Sink Connector.
With those sink connectors, data is taken from a Pulsar topic and put into {cass-short}.
With {cdc_cass}, updates to a {cass-short} table are converted into events and put into a data topic.
From there, the data can be published to external platforms like Elasticsearch, Snowflake, and other platforms.

== How do I install {cdc_cass}?

Follow the xref:install.adoc[installation instructions].

== What are the requirements for {cdc_cass}?

See the xref:ROOT:install.adoc[installation instructions].

== I have multiple {cass-short} datacenters. How do I configure {cdc_cass}?

See xref:ROOT:index.adoc#multiple-cassandra-datacenters[Deploy multiple {cass-short} datacenters].

== What is the impact of the {cass-short} CDC solution on the existing {cass-short} cluster?

For each CDC-enabled {cass-short} table, {cass-short} needs extra processing cycles and storage to process the CDC commit logs.
The impact for dealing with a single CDC-enabled table is small, but when there are a large number of {cass-short} tables with CDC enabled, the impact within {cass-short} increases.
The performance impact occurs within {cass-short} itself, not the {cass-short} CDC solution with Pulsar.

The {cdc_agent} is started as a JVM agent of the {cass-short} process and it shares the same hardware resource of the same {cass-short} node.
However, the only job that the {cdc_agent} does is to scan the CDC commit log directory on a regular basis and send messages to the Pulsar cluster.
This is a lightweight process when launched on a single thread, but the {cdc_agent} can be launched with multiple threads.
As more threads are launched, more resources are consumed.

For each {cass-short} write operation (one detected change-event), the Pulsar CSC connector performs a primary key-based {cass-short} read to get the most complete, up-to-date information of that particular {cass-short} row.

In a worst-case scenario, where a CDC-enabled {cass-short} has 100% write workload, the CDC solution would double the workload by adding the same amount of read workload to {cass-short} table.
Since the {cass-short} read is primary key-based, it is efficient.

== What are the {cdc_cass} limitations?

See xref:ROOT:index.adoc#limitations[{cdc_cass} limitations].

== What happens if the Apache Pulsar service is unavailable?

If the Pulsar cluster is down, the {cdc_agent} on each {cass-short} node attempts to send the mutations periodically, and it keeps the CDC commitlog segments on disk until the data sending is successful.

The {cdc_agent} keeps track of the CDC commitlog segment offsets, so the {cdc_agent} knows where to resume sending the mutation messages when the Pulsar cluster is back online.

{company} recommends active monitoring of the disk space of the {cass-short} nodes.
If the Pulsar cluster is down, the change agent continues trying to send messages, and the CDC commitlog files accumulate on the {cass-short} node. If the maximum CDC directory disk space is reached, future {cass-short} writes to the CDC-enabled table will fail.

When the disk space of the `cdc_raw` directory reaches your `cdc_total_space_in_mb` {cass-short} setting (less than 4 GB by default), writes to CDC-enabled tables fail with a `CDCWriteException`.
The following warning message is included in {cass-short} logs:

[source,bash]
----
WARN  [CoreThread-5] 2021-10-29 09:12:52,790  NoSpamLogger.java:98 - Rejecting Mutation containing CDC-enabled table. Free up space in /mnt/data/cdc_raw.
----

To avoid or recover from this situation, increase the `cdc_total_space_in_mb` and restart the node.
To prevent hitting this new limit, increase the write throughput to your Apache Pulsar cluster, or decrease the write throughput to your node.

Increasing the write throughput can involve tuning one or more of the following:

* Change agent configuration: The number of allocated threads, the batching delay, the number of inflight messages
* Pulsar cluster configuration: The number of partitions of your topics
* {csc_pulsar} configuration: The query executors, batching and cache settings, connector parallelism

As a last resort, if losing data is acceptable in your CDC pipeline, remove `commitlog` files from the `cdc_raw` directory.
Restarting the node isn't needed in this case.

== How do I know if CDC is enabled on a table?

You can check the CDC status of a table by running the following CQL query:

[source,cql]
----
SELECT * FROM system_distributed.cdc_local WHERE keyspace_name = 'keyspace_name' AND table_name = 'table_name';
----

There are three possible statuses:

Enabled::
If the CDC status is `enabled`, then CDC is enabled on the table.
+
From this status, you can disable CDC on the table by running the following CQL query:
+
[source,cql]
----
ALTER TABLE keyspace_name.table_name WITH cdc = {'enabled': false};
----

Disabled::
If the CDC status is `disabled` then CDC is disabled on the table.
+
From this status, you can enable CDC on the table by running the following CQL query:
+
[source,cql]
----
ALTER TABLE keyspace_name.table_name WITH cdc = {'enabled': true};
----

Null::
If the CDC status is `null` then CDC isn't enabled on the table.
+
From this status, you can enable CDC on the table by running the following CQL query:
+
[source,cql]
----
ALTER TABLE keyspace_name.table_name WITH cdc = {'enabled': true};
----

== How do I know if the {cdc_agent} is running?

You can check the status of the {cdc_agent} by running the following CQL query:

[source,cql]
----
SELECT * FROM system_distributed.cdc_local WHERE keyspace_name = 'cdc' AND table_name = 'raw_cdc';
----

There are three possible statuses:

Running::
If the `status` column is `running`, then the agent is running.
+
From this status, you can stop the agent by running the following CQL query:
+
[source,cql]
----
ALTER TABLE cdc.raw_cdc WITH cdc = {'enabled': false};
----

Stopped::
If the `status` column is `stopped` then the agent isn't running.
+
From this status, you can start the agent by running the following CQL query:
+
[source,cql]
----
ALTER TABLE cdc.raw_cdc WITH cdc = {'enabled': true};
----

Null::
If the `status` column is `null`, then the agent isn't running.
+
From this status, you can start the agent by running the following CQL query:
+
[source,cql]
----
ALTER TABLE cdc.raw_cdc WITH cdc = {'enabled': true};
----

== What happens to unacknowledged event messages the {cdc_agent} cannot deliver?

Unacknowledged messages mean the {cdc_agent} couldn't produce the event message in Pulsar.
In this case, the table row mutation fails.
The {cass-short} client handles this as an exception.
The data is committed to {cass-short} and no event is created.

Another scenario might be the Pulsar broker is too busy to process messages and a backlog has been created.
In this case, Pulsar's backlog policies take effect and event messages are handled accordingly.
The data is committed to {cass-short} but there might be some additional latency to the event message creation.

The design of CDC in {cass-short} assumed that when table changes are synchronized to the `raw_cdc` log, another process is draining that log.
There is a max log size setting that disables writes to the table when the set threshold is reached.
If a connection to the Pulsar cluster is needed for the log to be drained, and it isn't responsive, then the log begins to fill, which can impact a table's write availability.

For more, see the xref:cdc-for-cassandra:ROOT:install.adoc#scaling-up-your-configuration[Scaling up your CDC configuration].

== Does the {csc_pulsar_first} use a dead-letter topic?

A dead letter topic is used when a message cannot be delivered to a consumer.
Possible causes include the following:

* The message acknowledgment time expired with no consumer acknowledged receipt of the message
* A consumer negatively acknowledged the message
* A retry letter topic is in use and retries were exhausted

The {csc_pulsar} creates a consumer to receive new event messages from the {cdc_agent}, but doesn't configure a dead letter topic.
It is assumed that parallel instances, broker compute, and function worker compute are sized to handle the workload.

== How do I scale CDC to handle my production loads?

There are 3 areas of scalability to focus on:

Hosts in the {cass-short} cluster::
The {cdc_agent} is running on each host in its own JVM.
If you are administering your own {cass-short} cluster, then you can tune the JVM compute properties to handle the appropriate workload.
If you are using {cass-short} in a serverless environment, then the JVM is already set to handle significant load.

Number of {csc_pulsar} instances that are running::
This is initially set when the {cass-short} source connector is created, and it can be updated throughout the life of the running connector.
Depending on your Pulsar configuration, an instance can represent a process thread on the broker or a function worker.
If using Kubernetes, this could be a pod.
Each represents different scaling strategies like increasing compute, adding more workers, and more K8s nodes.

Broker backlog size and throughput tolerances::
There are potentially a large amount of messages being created, so you must ensure the Pulsar cluster is sized correctly.
For more information, see xref:luna-streaming:install-upgrade:production-cluster-sizing.adoc[].

== How do I filter table data by column?

xref:luna-streaming:operations:functions.adoc[Transformation functions] are a great way to manipulate messages on CDC data (with no code required).
Put them inline to watch the data topic and write to a different topic.
Name the topic something memorable, like `filtered-data` topic.

[#how-do-i-configure-multi-region-cdc-using-the-cassandra-sink]
== How do I configure multi-region CDC using the {cass-short} sink?

One of the requirements for CDC is that both the {cass-short} and Pulsar clusters must be in the same cloud region or on-premise data center.
If you are using geo-replication, you need the change data to be replicated across multiple clusters.
The most manageable way to handle this is to use Pulsar's {cass-short} sink to "watch" the CDC data topic and write the change to a different {cass-short} table in another organization.

The {cass-short} sink requires the following provisions:

- Use the CDC data topic as its source of messages
- Provide a secure bundle (creds) to another {cass-short} cluster
- Map message values to a specific table in the other cluster
- Use Pulsar's delivery guarantee to ensure success
- Use Pulsar's connector health metrics to monitor failures

== How do I migrate table data using CDC?

Migrating data between tables solves quite a few different challenges.
The basic approach is to use a {cass-short} sink to watch the {cass-short} source and write to another table while mapping columns appropriately.
As the original table is phased out, the number of messages decreases to none, while consumers are watching the new table's CDC data topic.
For more information, see <<how-do-i-configure-multi-region-cdc-using-the-cassandra-sink>>.